{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cardiovascular-breed",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/howesa/CHI22-CogMod-Tutorial/blob/main/04-Workflow/04-Workflow.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e1a00a",
   "metadata": {},
   "source": [
    "   \n",
    "# MODULE 4: Modeling Workflow\n",
    "In this module, we cover the modeling workflow from defining the model, running simulations, evaluating the model, and using it for optimization. The learning outcomes of this module are:\n",
    "\n",
    "* Understand modeling workflow, with an ability to provide a detailed case example\n",
    "* Be able to use a reinforcement learning model of multitasking while driving\n",
    "* Be able to use Bayesian likelihood free inference to fit model parameters\n",
    "* Understand how parameterized simulation models can be used to explore design candidates\n",
    "\n",
    "Why workflow? 1. Clear plan (knowing what to do, avoiding hacks). 2. Reproducability (clarity, transparency)\n",
    "\n",
    "Modeling workflows can be complicated processes, but they generally all involve the following overall cycle.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/04-Workflow/workflow_basic.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "This image is adapted from https://www.annualreviews.org/doi/10.1146/annurev-statistics-022513-115657 \n",
    "For an example of a more specific modeling workflow, you can read this Wilson & Collins 2019 paper: https://elifesciences.org/articles/49547\n",
    "\n",
    "**Note**. The word \"model\" has multiple meanings. A model can refer to the whole interactive task, including the user, or it can refer to just the user's cognition, or it can even refer to a specific internal model that the user has of the task. In this notebook, from now on, we will be using the following exact (but narrow) definitions.\n",
    "\n",
    "* An *agent* takes actions to reach goals.\n",
    "* The agent utilises a *policy*, which tells what the agent does in a particular state.\n",
    "* The agent is expected to follow a *bounded optimal policy*, meaning it generally does what is best for it in the long term.\n",
    "* The agent interacts with an *environment*, which for our computational is formalised. The resulting interaction of the formalised environment and the agent is called *simulation* of the interactive task.\n",
    "* The agent has an internal representation of the dynamics of the environment. Here, we call that a *model*, which, given the current state of the environment and an action taken by the agent, provides a prediction of what the next state will be.  \n",
    "\n",
    "This module will have the following structure, illustrated in the form of a potential workflow when creating new models.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/04-Workflow/workflow.png\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3770d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block of code first if you are on Colab!\n",
    "# ! wget https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/04-Workflow/driver.py\n",
    "# ! wget https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/04-Workflow/driver_agent.py\n",
    "# ! wget https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/04-Workflow/driver_env.py\n",
    "# ! wget https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/04-Workflow/search.py\n",
    "# ! wget https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/04-Workflow/search_agent.py\n",
    "# ! wget https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/04-Workflow/multitasking.py\n",
    "# ! wget https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/04-Workflow/multitasking_agent.py\n",
    "# ! wget https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/04-Workflow/precomputed_pred.py\n",
    "# ! wget https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/04-Workflow/run_experiment.py\n",
    "    \n",
    "# ! pip install stable_baselines3\n",
    "# ! pip install elfi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e89e44",
   "metadata": {},
   "source": [
    "# 1. Build The Model\n",
    "\n",
    "## 1.1. Task: Multitasking while Driving\n",
    "The module will make use of a computational cognitive model that simulates driver multitasking (Jokinen, Kujala, & Oulasvirta 2021: https://journals.sagepub.com/doi/10.1177/0018720820927687\n",
    "\n",
    "The model architecture takes the form of hierarchical control system, where different parts of the hierarchy are responsible for managing one part of the overall task. As illustrated above, the task is accomplished by three components.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/04-Workflow/model.png\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "Each component in the cognitive model is its own RL agent, responsible only for its own task: driving (the primary task), in-car visual search (the secondary task), or supervision (deciding which task to attend). The isolation of these agents makes it possible to define each one separately from all others. Next, we define the driving task by following the modeling workflow outlined in the figure.\n",
    "\n",
    "Driving in a naturalistic setting is a complicated task, which involves a large number of relevant environmental features, such as details about the road, other traffic, pedestrians, street signs, driving speed, orientation of the steering wheel, amount of fuel left, etc. In order to simplify the task into a manageable simulation, we here reduce the task to its simplest form. We define driving as a control task, where the car has a position on the lane. Depending on the speed $S$ of the car and angle of the steering wheel $\\omega$, the next position $x$ of the car on the lane is dictated by its previous position.\n",
    "\n",
    "$$ x_{t+1} = x_t + sin(\\omega) \\cdot S \\cdot \\tau, $$\n",
    "\n",
    "where $\\tau$ is a constant indicating the length of one tick of the simulation in seconds.\n",
    "\n",
    "## 1.2. Driver Agent\n",
    "\n",
    "We can then define the driver *agent*. At any time step $t$ the agent can take actions $\\omega_t \\in [\\omega_{min},\\omega_{max}]$ to steer the car. It receives a real-valued reward based on the position of the car on the lane: if the position at a time step $x_t$ is less than $x_min$ or more than $x_max$, the agent receives a negative reward. Otherwise it receives a reward of zero.\n",
    "\n",
    "We can now implement a simplified version of the driving environment and agent. Please note that this is not the full model, but is used here for demonstrative purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e316e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the driver from the scratch\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class driver(Env):\n",
    "    def __init__(self, speed = 17, oob_reward = -1):\n",
    "        self.speed = speed # m/s\n",
    "        self.oob_reward = oob_reward # reward from lane violation\n",
    "        \n",
    "        self.threshold = 0.2 # oob\n",
    "        \n",
    "        self.step_time = 0.1 # seconds\n",
    "        \n",
    "        # Steering wheel extrema\n",
    "        self.max_steer = 0.025\n",
    "        # How many discrete steering actions\n",
    "        self.actions = 10\n",
    "        # Action related noise\n",
    "        self.action_noise = 0\n",
    "        \n",
    "        # Steering noise\n",
    "        self.steer_noise = 0\n",
    "        \n",
    "        self.action_space = Discrete(self.actions)\n",
    "        \n",
    "        # Observe current lane position\n",
    "        self.observation_space = Box(low=0, high=1, shape = (1,))\n",
    "        \n",
    "        self.log = False\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.x = 0.5 # in the middle of the lane\n",
    "        self.time = 0\n",
    "        self.trace = {}\n",
    "        self.trace[\"action\"] = []\n",
    "        self.trace[\"x\"] = []\n",
    "        return [self.x]\n",
    "    \n",
    "    # Given a discrete action, return a true steering position\n",
    "    def action_to_steer(self, action):\n",
    "        a = self.action_space.n/2\n",
    "        return (action-a)*self.max_steer/a\n",
    "    \n",
    "    def update_car_pos(self, action):\n",
    "        steer = self.action_to_steer(action)\n",
    "        # Add action related noise\n",
    "        steer += abs(steer)*np.random.logistic(0, self.action_noise)\n",
    "        # Add non-action related noise.\n",
    "        steer += np.random.logistic(0, self.steer_noise)\n",
    "\n",
    "        # Limit steer\n",
    "        steer = min(max(steer, -self.max_steer), self.max_steer)\n",
    "        self.x += math.sin(steer) * self.speed * self.step_time\n",
    "        \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        \n",
    "        self.update_car_pos(action)\n",
    "        \n",
    "        if self.log:\n",
    "            self.trace[\"action\"].append(action)\n",
    "            self.trace[\"x\"].append(self.x)\n",
    "        \n",
    "        if self.x < self.threshold or self.x > 1 - self.threshold:\n",
    "            reward = self.oob_reward\n",
    "        else:\n",
    "            reward = 0\n",
    "            \n",
    "        # Limit x\n",
    "        self.x = max(min(self.x,1),0)\n",
    "\n",
    "        return [self.x], reward, done, {}\n",
    "    \n",
    "    def plot_trace(self):\n",
    "        plt.close()\n",
    "        plt.ylim(0,1)\n",
    "        ax = plt.gca()\n",
    "        ax.invert_yaxis()\n",
    "        oob = np.greater(self.trace[\"x\"], 1 - self.threshold) + np.greater(self.threshold, self.trace[\"x\"])\n",
    "        t = np.linspace(0,len(self.trace[\"x\"])*self.step_time, len(self.trace[\"x\"]))\n",
    "        plt.scatter(t, self.trace[\"x\"], c = oob)\n",
    "        \n",
    "    def summarize_trace(self):\n",
    "        ret = {}\n",
    "        ret[\"sd\"] = np.std(self.trace[\"x\"])\n",
    "        outside = np.sum(np.greater(self.trace[\"x\"], 1 - self.threshold))\n",
    "        outside += np.sum(np.greater(self.threshold, self.trace[\"x\"]))\n",
    "\n",
    "        ret[\"oob\"] = outside / len(self.trace[\"x\"])\n",
    "        \n",
    "        ret[\"sd_of_action\"] = np.std(self.trace[\"action\"])\n",
    "        \n",
    "        return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a94645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try the driving task!\n",
    "\n",
    "d = driver(17)\n",
    "d.log = True\n",
    "d.reset()\n",
    "for i in range(10):\n",
    "    d.step(1) # hard left steer\n",
    "for i in range(10):\n",
    "    d.step(9) # hard steer right\n",
    "for i in range(10):\n",
    "    d.step(5) # drive straight\n",
    "\n",
    "d.plot_trace()\n",
    "d.summarize_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729907d6",
   "metadata": {},
   "source": [
    "Now that a very simple driving environment and agent have been defined, we can train a RL model. We'll use PPO for faster convergence than the original paper, which used tabular Q learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8efc018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "d.log = False\n",
    "d_agent = PPO(\"MlpPolicy\", d, verbose = 0)\n",
    "\n",
    "d_agent.learn(total_timesteps = 10000) # This is going to take about 20s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a trace from the agent.\n",
    "def simulate_driver(driver, agent, timesteps = 1000):\n",
    "    driver.log = True\n",
    "    state = driver.reset()\n",
    "    for i in range(timesteps):\n",
    "        action, _ = agent.predict(state)\n",
    "        state , _, _, _ = driver.step(action)\n",
    "    driver.plot_trace()\n",
    "    driver.log = False\n",
    "    return driver.summarize_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_driver(d, d_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3d0695",
   "metadata": {},
   "source": [
    "## 1.3 Assignment 1\n",
    "Change the speed to 120km/h (but note it must be expressed in m/s) and plot a similar simulation. Compare the traces and summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b484a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1.\n",
    "d2 = driver(17) # TODO: change speed!\n",
    "d2_agent = PPO(\"MlpPolicy\", d2, verbose = 0)\n",
    "# TODO: Train the agent\n",
    "# TODO: produce simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ade234",
   "metadata": {},
   "source": [
    "## 1.4 Introducing Bounds\n",
    "\n",
    "Now that the environmental dynamcis and the agent's goals and available actions have been defined, we can ask: how to make the RL agent behave more like a human? To that end, we should specify bounds.\n",
    "\n",
    "Noise is one typical source of bounds for natural agents. The driver model has steering noise, which models the noise inherent in human motor control. We use the `steer_noise` parameter to model this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa4d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = driver(17)\n",
    "d.steer_noise = 0.02\n",
    "d_agent = PPO(\"MlpPolicy\", d, verbose = 0)\n",
    "\n",
    "d_agent.learn(total_timesteps = 50000) # This is going to take about a minute.\n",
    "simulate_driver(d, d_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c77dc3",
   "metadata": {},
   "source": [
    "Including steering related noise, the complete model of multitasking while driving has the following bounds.\n",
    "\n",
    "* Steering noise: the steering wheel's angle has noise.\n",
    "* Action related noise: larger steering actions are associated with more noise.\n",
    "* Limited attentional capacity, the driver must choose wether to attend the road or the in-car task, at the cost of losing vision in the other.\n",
    "* Foeveated vision: when conducting the secondary task, the user needs to move eyes between the elements of the in-car UI in order to encode the elements. Eye movements and encoding takes time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542afbcd",
   "metadata": {},
   "source": [
    "The full model consists of a slighly more complicated driving model, a search model, and a supervisory model. We don't have time to go into the details here, but the full implementation can be loaded as is done below. Please note that these are not the original models as reported in the paper, but instead use PPO for faster convergence.\n",
    "\n",
    "The idea in this model is that the subtask agents - driving and search - learn to complete their tasks in isolation (the driver is trained so that its vision is occluded from time to time). Then, a supervisory model is trained by letting it allocate visual attention between the subtask models, and observing what values the subtask models predict as a result of this choice. The supervisor is rewarded as the joint reward from the two subtasks.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/04-Workflow/model_detail.png\" width=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bb46200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_experiment\n",
    "import multitasking_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f5386",
   "metadata": {},
   "source": [
    "# 2. Forward Modeling\n",
    "\n",
    "Now that we have defined the full model architecture for multitasking while driving, we can use it to produce simulated results and evaluate the face-validity of these results. We will be focusing on two face-valid hypotheses. First, increasing driving speed should result in shorter in-car glance duration. Second, prioritising driving safety should result in more time spent driving at the cost of the secondary task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b305cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed: car speed in m/s\n",
    "# obs_prob: visibility [0,1]\n",
    "# an: noise that is proportional to steering actions\n",
    "# sn: noise in steering\n",
    "# or: out of bound reward, i.e., penalty for lane excursions\n",
    "# cols, rows: the dimensions of the in-car device (how many elements)\n",
    "# fr: how rewarding it is to find targets in the in-car device\n",
    "params = {\"speed\": 17,\n",
    "          \"obs_prob\": 0.8,\n",
    "          \"an\": 0.01,\n",
    "          \"sn\": 0.01,\n",
    "          \"or\": -1,\n",
    "          \"cols\": 3,\n",
    "          \"rows\": 3,\n",
    "          \"fr\": 10}\n",
    "\n",
    "# Running this will take about 10 minutes.\n",
    "trace = run_experiment.run_experiment(params, 1200, max_iters = 20)\n",
    "print(multitasking_agent.summarise_trace(trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddef1118",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "Change the agent's preferences regarding safe driving vs. sucessful in-car interaction. Increase the penalty received from the lane boundary violation from -1 to e.g., -5, and see what happens. Explain the differences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c4c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 2\n",
    "params2 = {} # TODO: set params correctly\n",
    "# TODO: Create trace\n",
    "# TODO: Summarize trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76f15dd",
   "metadata": {},
   "source": [
    "# 3. Inference\n",
    "\n",
    "*Parameter inference* refers to identifying values that are theoretically plausible and lead to realistic predictions. This is an *inverse modeling* problem: given observed behaviour, what are the plausible parameter values? In forward modeling, we pass a set of parameters $\\theta$ to a model $M$, which then outputs data predictions $D_p$. In inverse modeling, we have observed read data $D_o$, and the goal is to find out a plausible set of parameters $\\theta$, that with the model produce predictions that are close to the observations.  \n",
    "\n",
    "<div>\n",
    "<img src=\"inverse.png\" width=\"200\">\n",
    "</div>\n",
    "\n",
    "An in-press book chapter by Jokinen et al. https://www.jyu.fi/it/fi/tiedekunta/henkilosto/henkilosto/jokinen-jussi/jokinen-2021-bayesian.pdf has a thorough explanation of inverse modeling and Bayesian inference for cognitive models in HCI. Another paper by Kangasrääsiö et al https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12738 provides in-depth walkthroughs on how to infer parameters of cognitive models. Here, we will briefly cover the main points.\n",
    "\n",
    "The main reason for using Bayesian parameter inference for inverse modeling over other possible approaches is that it provides a **mathematically rigorous framework for evaluating the plausibility of different parameter values**. Bayesian parameter inference is grounded on the fact that we generally cannot know the exact parameter values that best describe the observed data and user(s) behind it. However, it is possible to obtain information about the parameter values, and this information can be represented as a probability distribution over the possible parameter values. Possible information about parameters $\\theta$ includes both what can be learned based on observations $D_o$, and our expectations about plausible parameter values based on what we know about the simulator model.\n",
    "\n",
    "The idea is to express, prior to making observations, what we know about the parameters as a distribution $P(\\theta)$, and then make observations to update this expectation.\n",
    "Given the prior probabilities $P(\\theta)$ and observation likelihood $P(D_o|\\theta)$, posterior probabilities are defined as\n",
    "\n",
    "$$\n",
    "  P(\\theta \\mid D_O) = \\frac{P(D_O \\mid \\theta) P(\\theta)}{P(D_O)},\n",
    "$$\n",
    "\n",
    "where $P(D_o)$ is the marginal likelihood $P(\\theta)=\\int P(D_o|\\theta)P(\\theta)d\\theta$. The posterior $P(\\theta|D_o)$ is a probability distribution over parameter values, and it represents what we know about the unknown parameters when we take into account that these parameters produced the observations $D_o$.\n",
    "\n",
    "We will be using Bayesian likelihood-free parameter inference using the ELFI package (https://elfi.readthedocs.io/en/latest/). \n",
    "\n",
    "## Inferring Individual Driving Ability \n",
    "\n",
    "The model for multitasking while driving contains multiple parameters. Most of them are specified based on the description of the task environment (e.g., speed), some are based on literature (e.g., eye movement times), but some can be used to describe individuals. For instance, action related noise can be argued to be a parameter, that varies in the population. Perhaps age, or motor control problems, are associated with it. Or, it could simply be that more experienced drivers have less noisy behaviours.\n",
    "\n",
    "Again, we start with the simple driving model created. For the purposes of this execrise, we will be using simulated data, not data from real humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c156d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import elfi\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "import precomputed_pred\n",
    "data = precomputed_pred.data\n",
    "\n",
    "# Use parametrised RL model of driving to generate summaries.\n",
    "# Optionally, supply a precomputed dataset that maps parameter \n",
    "# values to summaries.\n",
    "def generate_driving_data(steer_noise, batch_size = 1, random_state = None):\n",
    "    if data:\n",
    "        arr = np.array(list(data.keys()))        \n",
    "        d_pred = data[arr[np.abs(arr - steer_noise[0]).argmin()]] # find closest match\n",
    "    else:\n",
    "        d_agent = PPO(\"MlpPolicy\", d, verbose = 0)\n",
    "        d = driver(17)\n",
    "        d.action_noise = 0.02 # use default for this exercise\n",
    "        d.steer_noise = steer_noise\n",
    "        d_agent = PPO(\"MlpPolicy\", d, verbose = 0)\n",
    "        d_agent.learn(total_timesteps = 10000)\n",
    "        d_pred = simulate_driver(d, d_agent)\n",
    "    return d_pred\n",
    "    \n",
    "# Create the ELFI model\n",
    "elfi.new_model()\n",
    "# Almost uninformed prior\n",
    "steer_noise = elfi.Prior(scipy.stats.uniform, 0, 0.1)\n",
    "# Observations. We use imaginary observations, but in reality this would be human data.\n",
    "obs_a = {'sd': 0.15, # note: we are using fairly large sd for simple driving\n",
    "         'oob': 0.01,\n",
    "         'sd_of_action': 3.0}\n",
    "\n",
    "# The ELFI model\n",
    "Y = elfi.Simulator(generate_driving_data, steer_noise, observed = obs_a)\n",
    "\n",
    "# Summary statistics. As our data are already summarised, return it.\n",
    "# If either/both observed and predicted data were not summarised,\n",
    "# we would do that here.\n",
    "def sd_of_x(d):\n",
    "    return d[\"sd\"]\n",
    "\n",
    "S1 = elfi.Summary(sd_of_x, Y)\n",
    "d = elfi.Distance('euclidean', S1)\n",
    "\n",
    "bolfi = elfi.BOLFI(d, batch_size=1, initial_evidence=20,update_interval=10,\n",
    "                       bounds={'steer_noise':(0,0.1)}, seed = seed)\n",
    "\n",
    "\n",
    "bolfi.fit(n_evidence=100)\n",
    "\n",
    "bolfi.plot_discrepancy();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd7039",
   "metadata": {},
   "source": [
    "The figure above shows us the discrepancy between the observed and simulated data, with smaller values indicating a better fit. The fitted ELFI model can now be sampled in order to produce a posterior for the parameter. While the discrepancy illustrated above can already be used to inspect the most plausible parameter values, the posterior provides us with a way to assess our confidence in this inference, as well as incorporate the prior.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaee74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a sample of the posterior\n",
    "result = bolfi.sample(500, info_freq=500)\n",
    "\n",
    "result.plot_marginals()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd016f",
   "metadata": {},
   "source": [
    "This posterior can be freely analysed. A point estimate, such as mean, median, or mode, can be used to set the most plausible parameter value, i.e., the value that best produces the observed behaviour, when used to parametrize the simulation model. Entropy or standard deviation can be used to assess our confidence in that point estimate, which should therefore not be used \"blindly\" as the best fit. Finally, one can sample the posterior for multiple values for the parameter, and use this sample to generate a range of predictions form the simulation model. The resulting *posterior predictive distribution* is useful in predicting the range of behaviours, given the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef09883",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(result.samples['steer_noise']))\n",
    "print(np.median(result.samples['steer_noise']))\n",
    "print(np.std(result.samples['steer_noise']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81843c4d",
   "metadata": {},
   "source": [
    "The whole process just done can be illustrated using this figure (from the cited Jokinen et al. in press paper). The key difference between the left and right pane is in how the model is parametrized: in the left, we use a point estimate (e.g., mean of the posterior). In the right, we sample the posterior. The latter method expresses uncertainty associated with the inference in the range of predictions from the model. It also permits assessing the probability of \"tail cases\", which may be of specific interest to the workflow. For instance, we might be interested in the probability of a fatal car crash - even if such a probability is generally relatively small.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/04-Workflow/distributions.png\" width=\"800\">\n",
    "</div>\n",
    "\n",
    "**(a)** A simulator $M$ with fixed parameters $\\theta$ can be run multiple times to produce a series of predictions $D_p$, which are each summarised, resulting in a distribution of summaries.\n",
    "**(b)** When a posterior of parameters $\\theta$ has been created using Bayesian parameter inference, a posterior predictive distribution can be generated by repeatedly sampling values of $\\theta$ from the posterior and summarising the resulting predictions $D_p$. Note that posterior predictive inference may be able to more plausibly estimate the occurrence of tail cases of some model features. Concentrations of observable model features in (b) illustrate that the simulator with fixed parameters can underestimate the amount of tail cases.\n",
    "**(c)** Observing a process in the world $W$ produces data $D_o$ that can then be summarised using summary statistics $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe0ad62",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "Infer the posterior for the second human participant, obs_b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c1e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 3\n",
    "obs_b = {'sd': 0.21,\n",
    "         'oob': 0.02,\n",
    "         'sd_of_action': 3.5}\n",
    "# TODO: The elfi pipeline. Note that aside from the observations, everything else can be reused.\n",
    "# But please make sure you at least try to understand what is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c018c0",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93ed7d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
