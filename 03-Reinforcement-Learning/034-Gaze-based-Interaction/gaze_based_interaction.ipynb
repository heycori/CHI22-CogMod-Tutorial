{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fitted-component",
   "metadata": {},
   "source": [
    "# Computationally rational gaze-based interaction\n",
    "\n",
    "Andrew Howes & Xiuli Chen\n",
    "\n",
    "The purpose of this tutorial is to introduce an approach to building computationally rational models.\n",
    "\n",
    "It does the following:\n",
    "\n",
    "* imports libraries,\n",
    "* defines a cognitive POMDP for computational rationality,\n",
    "* defines a theory of gaze based interaction as a cognitive POMDP,\n",
    "* defines auxiliary assumptions (the task),\n",
    "* combines the theory and auxiliary assumptions into a machine learning problem (a model) that can be solved with baselines3,\n",
    "* train the model,\n",
    "* examines the learning curve to ensure that we are generating an approximately optimal policy,\n",
    "* animates the model behaviour to develop our intuitions about its adaptation,\n",
    "* compares the model to human data.\n",
    "\n",
    "Preqrequisites:\n",
    "\n",
    "* foveated vision notebook\n",
    "* Bayesian integration notebook\n",
    "* POMDP notebook\n",
    "* Q-learning notebook\n",
    "* PPO notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "obvious-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "\n",
    "from gazetools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-leadership",
   "metadata": {},
   "source": [
    "### A Cognitive POMDP\n",
    "\n",
    "A cognitive POMDP is a framework for specifying 'cognitive problems'. Cognitive problems are theories of the problems faced by cognition.\n",
    "\n",
    "If we assume that people are computationally rational then the optimal solution to a cognitive problem predicts human behavior.\n",
    "\n",
    "\n",
    "<img src=\"image/internal_POMDP.png\" width=400>\n",
    "\n",
    "We can write a program that formalises the cognitive problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "monetary-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CognitiveTheory():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.internal_state = {}\n",
    "        \n",
    "    def step(self, aux, decision):\n",
    "        ''' Define the cognitive POMDP.'''\n",
    "        self._update_state_with_decision(decision)\n",
    "        action = self._get_action()\n",
    "        external_state, done = aux.external_env(action)\n",
    "        percept, percept_std = self._get_percept(aux.external_state)\n",
    "        self._update_state_with_percept(percept, percept_std)\n",
    "        obs = self._get_obs()\n",
    "        reward = self._get_reward()\n",
    "        return obs, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-timeline",
   "metadata": {},
   "source": [
    "### A theory of gaze-based interaction\n",
    "\n",
    "Each of the entities in CognitiveTheory must be defined so as to state our theory of gaze-based interaction. The theory makes the following assumptions:\n",
    "\n",
    "* Target location percepts in human vision are corrupted by Gaussian noise.\n",
    "* The standard deviation of noise increases linearly with eccentricity from the fovea.\n",
    "* Sequences of noisy percepts are optimally integrated.\n",
    "* Intended eye movements (oculomotor decisions) are corrupted by signal dependent Gaussian noise.\n",
    "\n",
    "These assumptions are further described in Chen et al. (2021)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accepted-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GazeTheory(CognitiveTheory):\n",
    "\n",
    "    def __init__(self):\n",
    "        ''' Initialise the theoretically motivated parameters.'''\n",
    "        # weight eye movement noise with distance of saccade\n",
    "        self.oculamotor_noise_weight = 0.01\n",
    "        # weight noise with eccentricity\n",
    "        self.perceptual_noise_weight = 0.09\n",
    "        # step_cost for the reward function\n",
    "        self.step_cost = -1\n",
    "        # super.__init__()\n",
    "\n",
    "    def reset_internal_env(self, external_state):\n",
    "        ''' The internal state includes the fixation location, the latest estimate of \n",
    "        the target location and the target uncertainty. Assumes that there is no \n",
    "        uncertainty in the fixation location.\n",
    "        Assumes that width is known. All numbers are on scale -1 to 1.\n",
    "        The target_std represents the strength of the prior.'''\n",
    "        self.internal_state = {'fixation': np.array([-1,-1]),  \n",
    "                               'target': np.array([0,0]), \n",
    "                               'target_std': 0.1,# this should be set to the true target_std as defined in GazeTask.\n",
    "                               'width': external_state['width'],\n",
    "                               'decision': np.array([-1,-1])} \n",
    "        return self._get_obs()\n",
    "\n",
    "    def _update_state_with_decision(self, decision):\n",
    "        self.internal_state['decision'] = decision\n",
    "        \n",
    "    def _get_action(self):\n",
    "        ''' Take a decision and add noise.'''\n",
    "        # !!!! should take internal_state as parameter\n",
    "        move_distance = get_distance( self.internal_state['fixation'], \n",
    "                                     self.internal_state['decision'] )\n",
    "        \n",
    "        ocularmotor_noise = np.random.normal(0, self.oculamotor_noise_weight * move_distance, \n",
    "                                        self.internal_state['decision'].shape)\n",
    "        # action is decision plus noise\n",
    "        action = self.internal_state['decision'] + ocularmotor_noise\n",
    "        \n",
    "        # make an adjustment if action is out of range. \n",
    "        # This should be done somewhere other than in the theory.\n",
    "        action = np.clip(action,-1,1)\n",
    "        return action\n",
    "    \n",
    "    def _get_percept(self, external_state):\n",
    "        ''' define a psychologically plausible percept function in which acuity \n",
    "        falls off with eccentricity.''' \n",
    "        eccentricity = get_distance( external_state['target'], external_state['fixation'] )\n",
    "        prcpt_std = self.perceptual_noise_weight * eccentricity\n",
    "        perceptual_noise = np.random.normal(0, prcpt_std, \n",
    "                                         external_state['target'].shape)\n",
    "        prcpt = external_state['target'] + perceptual_noise\n",
    "        return prcpt, prcpt_std\n",
    "\n",
    "    \n",
    "    def _update_state_with_percept(self, percept, percept_std):\n",
    "        posterior, posterior_std = self.bayes_update(percept, \n",
    "                                                     percept_std, \n",
    "                                                     self.internal_state['target'],\n",
    "                                                     self.internal_state['target_std'])\n",
    "        self.internal_state['target'] = posterior\n",
    "        self.internal_state['target_std'] = posterior_std\n",
    "\n",
    "    def bayes_update(self, percept, percept_std, belief, belief_std):\n",
    "        ''' A Bayes optimal obs function that integrates multiple percepts.\n",
    "        The belief is the prior.'''\n",
    "        z1, sigma1 = percept, percept_std\n",
    "        z2, sigma2 = belief, belief_std\n",
    "        w1 = sigma2**2 / (sigma1**2 + sigma2**2)\n",
    "        w2 = sigma1**2 / (sigma1**2 + sigma2**2)\n",
    "        posterior = w1*z1 + w2*z2\n",
    "        posterior_std = np.sqrt( (sigma1**2 * sigma2**2)/(sigma1**2 + sigma2**2) )\n",
    "        return posterior, posterior_std\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        # the Bayesian posterior has already been calculated so just return it.\n",
    "        # could also return the target_std so that the controller knows the uncertainty \n",
    "        # of the observation.\n",
    "        #return self.internal_state['target']\n",
    "        return np.array([self.internal_state['target'][0],\n",
    "                        self.internal_state['target'][1],\n",
    "                        self.internal_state['target_std']])\n",
    "    \n",
    "    def _get_reward(self):\n",
    "        distance = get_distance(self.internal_state['fixation'], \n",
    "                                self.internal_state['target'])\n",
    "        \n",
    "        if distance < self.internal_state['width'] / 2:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = -distance # a much better model of the psychological reward function is possible.\n",
    "            \n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-missile",
   "metadata": {},
   "source": [
    "### Auxiliary assumptions\n",
    "\n",
    "In order to test the theory we need to make auxiliary assumptions. For the theory of gaze-based interaction presented here, the auxiliary assumptions mostly concern the task as defined in the external_environment. Auxiliary assumptions allow us to make predictions from the theory for a particular task. The theory makes predictions for many more tasks. For example, adaptation to mixed target widths and distances.\n",
    "\n",
    "Note auxiliary assumptions must not be \"auxiliary hypothesis\". See Gershman (2019) https://link.springer.com/article/10.3758/s13423-018-1488-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bacterial-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GazeTask():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.target_width = 0.15\n",
    "        self.target_loc_std = 0.3\n",
    "\n",
    "    def reset_external_env(self):\n",
    "        ''' The external_state includes the fixation and target location.\n",
    "        Choose a new target location and reset to the first fixation location.'''\n",
    "        \n",
    "        def _get_new_target():\n",
    "            x_target =np.clip(np.random.normal(0, self.target_loc_std),-1,1)\n",
    "            y_target =np.clip(np.random.normal(0, self.target_loc_std),-1,1)         \n",
    "            return np.array( [x_target, y_target] )\n",
    "    \n",
    "        fx = np.array([-1,-1])\n",
    "        tg = _get_new_target()\n",
    "        self.external_state = {'fixation':fx, 'target':tg, 'width':self.target_width }\n",
    "    \n",
    "    def external_env(self, action):\n",
    "        self.external_state['fixation'] = action\n",
    "        \n",
    "        # determine when the goal has been achieved.\n",
    "        distance = get_distance(self.external_state['fixation'], \n",
    "                                self.external_state['target'])\n",
    "        if distance < self.external_state['width']/2 :\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        return self.external_state, done\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-alliance",
   "metadata": {},
   "source": [
    "### Gym environment\n",
    "\n",
    "In order to find an optimal policy we use the theory and auxiliary assumptions to define a machine learning problem, here, making use of the framework defined by one specific library called gym.\n",
    "\n",
    "For further information see: https://gym.openai.com/\n",
    "\n",
    "gym.Env is a class provided by this library. Note that Env here refers to all of the components of the, including both internal and external environment, with the exception of the controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "applicable-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GazeModel(gym.Env): # also inherit from Alex's BaseModel in Corati.\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        def default_box(x):\n",
    "            return spaces.Box(low=-1, high=1, shape=(x, ), dtype=np.float64)\n",
    "        \n",
    "        self.GT = GazeTheory()\n",
    "        self.TX = GazeTask()        \n",
    "        \n",
    "        # Required by gym. These define the range of each variable.\n",
    "        # note that in gym an 'action' is a 'decision' in our framework.\n",
    "        # Each action has an x,y coordinate therefore the box size is 2.\n",
    "        # Each obs has a an x,y and an uncertainty therefore the box size is 3.\n",
    "        self.action_space = default_box(2)\n",
    "        self.observation_space = default_box(3)\n",
    "        \n",
    "        # max_fixations per episode. Used to curtail exploration early in training.\n",
    "        self.max_steps = 500\n",
    "        \n",
    "    def reset(self):\n",
    "        self.n_step = 0\n",
    "        self.TX.reset_external_env()\n",
    "        obs = self.GT.reset_internal_env( self.TX.external_state )\n",
    "        return obs\n",
    "    \n",
    "    def step(self, decision):\n",
    "        obs, reward, done = self.GT.step( self.TX, decision )\n",
    "        self.n_step+=1\n",
    "\n",
    "        # give up if been looking for too long\n",
    "        if self.n_step > self.max_steps:\n",
    "            done = True\n",
    "        \n",
    "        info = self.get_info()\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def get_info(self):\n",
    "        return {'step': self.n_step,\n",
    "                'target_width': self.TX.target_width,\n",
    "                'target_x': self.TX.external_state['target'][0],\n",
    "                'target_y': self.TX.external_state['target'][1],\n",
    "                'fixate_x':self.TX.external_state['fixation'][0],\n",
    "                'fixate_y':self.TX.external_state['fixation'][1] }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-covering",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "\n",
    "Step through the untrained model to check for simple bugs. More comprehensive tests needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ed00048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    }
   ],
   "source": [
    "model = GazeModel()\n",
    "\n",
    "model.reset()\n",
    "\n",
    "i=0\n",
    "done = False\n",
    "while not done:\n",
    "    # make a step with a randomly sampled action\n",
    "    obs, reward, done, info = model.step(model.action_space.sample())\n",
    "    i+=1\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-stevens",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "We can train the model to generate a controller.\n",
    "\n",
    "By plotting the learning curve we can see whether the performance improves with training and whether the model approaches an optimum performance. We are interested in approximately optimal performance, so if the training curve is not approaching asymptote then we need to train with more timesteps or revise the model.\n",
    "\n",
    "We can see that at first the model uses hundreds of fixations to find the target, this is because it has not yet learned to move the gaze in a way that is informed by the observation. As it learns to do this, it takes fewer steps to gaze at the target and its performance improves.\n",
    "\n",
    "If our problem definition is correct then the model will get more 'human-like' the more that it is trained. In other words, training makes it a better model of interaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d0910ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEJCAYAAACKWmBmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAg00lEQVR4nO3df1RUdeL/8efASEQIAgMRiilqKpZmGamViuKptR+2brFsZelauZJSuXXUj7JbYUf6QVn5i8zMys2s1LQ23SWtTFdjw8oyE0sLRMQB8RcMMDP3+4cf5+uvbowKl4+8Hud0jnMvd+5rfsSL9/vO3GszDMNARETkVwRYHUBERJo2FYWIiJhSUYiIiCkVhYiImFJRiIiIKRWFiIiYslsdoCGUlJSc9rYOhwOn03kW05wdyuUf5fKPcvnnXMwVFxf3q+s0ohAREVMqChERMaWiEBERUyoKERExpaIQERFTKgoRETGlohAREVMqChERMaWiEBERUyoKERExpaIQERFTKgoRETGlohAREVMqChERMaWiEBERUyoKERExpaIQERFTKgoRETGlohAREVMqChERMaWiEBERUyoKERExpaIQERFTKgoRETGlohAREVMqChERMaWiEBERUyoKERExpaIQERFTKgoRETGlohAREVMqChERMaWiEBERU3arAwDs3LmTuXPnUltbS2BgIPfeey8dO3akqqqKF198kfLycjweDzfffDPJyclWxxURaVaaRFG8+eab3HbbbfTs2ZOCggLefPNNHnvsMVauXEmbNm2YOHEiBw4c4MEHH+S6667Dbm8SsUVEmoUmMfVks9morq4GoKqqioiICN9yl8uFYRi4XC5CQ0MJCGgSkUVEmg2bYRiG1SGKi4t58sknAfB6vUydOpXo6Giqq6t5+umn2bVrF9XV1Tz88MNcccUVJ22fl5dHXl4eANnZ2dTW1p52FrvdjtvtPu3tG4py+Ue5/KNc/jkXcwUFBf3qukYriqysLCorK09anpaWxubNm0lMTKR3796sX7+ejz/+mMzMTDZs2MDWrVu555572LNnD1lZWTzzzDOEhISY7qukpOS0czocDpxO52lv31CUyz/K5R/l8s+5mCsuLu5X1zXaZH9mZuavrpsxYwYjR44EoE+fPuTm5gKwZs0abr31Vmw2G7GxscTExFBSUkLHjh0bJbOIiDSRYxSRkZFs2bIFgG+//ZbY2FjgSDtu3rwZgMrKSkpKSoiJibEsp4hIc9QkPj40evRo5s+fj9frpUWLFowePRqAP/zhD8yaNYu//vWvANx5552EhYVZGVVEpNlpEkXRpUsXnnrqqZOWR0ZGMmXKFAsSiYjIUU1i6klERJouFYWIiJhSUYiIiCkVhYiImFJRiIiIKRWFiIiYUlGIiIgpFYWIiJhSUYiIiCkVhYiImFJRiIiIKRWFiIiYUlGIiIgpFYWIiJhSUYiIiCkVhYiImFJRiIiIKRWFiIiYUlGIiIgpFYWIiJhSUYiIiCkVhYiImFJRiIiIKRWFiIiYUlGIiIgpFYWIiJhSUYiIiCkVhYiImFJRiIiIKRWFiIiYUlGIiIgpFYWIiJhSUYiIiCkVhYiImLJbHWDnzp3MnTsXl8tFdHQ0GRkZhISEALB06VJWr15NQEAAI0eO5PLLL7c2rIhIM2T5iCI3N5c777yTnJwckpKSWL58OQDFxcWsX7+e5557jsmTJzNv3jy8Xq/FaUVEmh/Li6KkpISuXbsC0L17dzZu3AhAfn4+ffv2pUWLFsTExBAbG8v27dutjCoi0ixZPvUUHx9Pfn4+SUlJbNiwgfLycgAqKiro1KmT7+ciIyOpqKg45X3k5eWRl5cHQHZ2Ng6H47Tz2O32M9q+oSiXf5TLP8rln+aWq1GKIisri8rKypOWp6WlMWbMGObPn897771Hr169sNv9j5SSkkJKSorvttPpPO2sDofjjLZvKMrlH+Xyj3L551zMFRcX96vrGqUoMjMzTddPmTIFODINVVBQABwZQRwdXcCREUZkZGTDhRQRkVOy/BjF/v37AfB6vSxZsoTBgwcD0KtXL9avX09dXR1lZWXs3r2bjh07WhlVRKRZsvwYxbp161i1ahUASUlJJCcnA0eOXfTp04fx48cTEBDAqFGjCAiwvNdERJody4tiyJAhDBky5JTrhg0bxrBhwxo5kYiIHEt/oouIiKl6jyiKi4sJDQ2lVatWuFwuli9fjs1m45ZbbuG8885ryIwiImKheo8oXnjhBaqqqgB4/fXX+f777yksLOTll19usHAiImK9eo8oysrKiIuLwzAMvvjiC5577jmCgoIYO3ZsQ+YTERGL1bsogoKCqK6upri4GIfDQVhYGB6Ph7q6uobMJyIiFqt3UVxzzTU88cQTVFdXc8MNNwCwY8cOYmJiGiyciIhYr95FMWLECL7++msCAwO59NJLAbDZbNxzzz0NFk5ERKzn1/coevTocdztDh06nNUwIiLS9JgWxd/+9jdsNttv3snjjz9+1gKJiEjTYloUAwcO9P17z549rFmzhv79+xMdHY3T6eTTTz/1nXJDRETOTaZFMWDAAN+/J0+ezOTJk4mPj/ctu/baa5k9ezapqakNFlBERKxV7y/cFRcXc+GFFx63LCYmhl27dp31UCIi0nTUuygSExOZNWsWu3fvpra2lpKSEmbPnk2XLl0aMp+IiFis3p96euCBB3jllVcYP348Xq+XwMBAkpKSSE9Pb8h8IiJisXoVhdfr5cMPPyQ9PZ2MjAwOHDhAWFiYrg8hItIM1Os3fUBAAP/617+w2+0EBATQqlUrlYSISDNR79/2/fr149///ndDZhERkSao3scotm/fzsqVK1m+fDlRUVHHfRFPX7gTETl31bsoBg0axKBBgxoyi4iINEH1Lopjv3wnIiLNh18nBaysrGT79u0cPHgQwzB8y4891YeIiJxb6l0UX3zxBS+99BIXXXQRRUVFxMfHU1RURJcuXc6ZovAumktFaTGeJngxpooWLZTLD8rlH+XyT1PNdfCSRBh611m/33oXxdtvv016ejp9+vRh5MiRPP3006xZs4aioqKzHkpERJqOeheF0+mkT58+xy3r378/999/P3ffffdZD2aFgLT7iHQ4cDqdVkc5iXL5R7n8o1z+aaq5Wjoc1DRArnp/jyIsLIzKykoAoqOj2bZtG3v27MHr9Z71UCIi0nT49fHYrVu30rt3b2688UYef/xxbDYbN910U0PmExERi9W7KG699Vbfv/v370+3bt1wuVy0adOmIXKJiEgTUe+pp//+978cPnzYd9vhcKgkRESagXqPKFasWMELL7xAbGwsiYmJJCYm0rVrV8LCwhoyn4iIWKzeRfH4449TW1tLYWEhW7ZsYdWqVcyYMYOYmBhycnIaMqOIiFjIr3OFe71e3G43dXV11NXVccEFF9C6deuGyiYiIk1AvUcUkyZNorKyks6dO5OYmMjo0aN1jEJEpBmo94giJCQEt9vN4cOHff95PJ6GzCYiIk1AvUcUmZmZeDwefvrpJ77//nuWLVvG9u3badu2LZmZmQ2ZUURELOTXMYrq6mr27dtHeXk5TqeTqqoqamtrGyqbiIg0AfUeUTzyyCOUlpbSoUMHunbtyvDhw+ncuTPnnXfeGQXYuXMnc+fOxeVyER0dTUZGBiEhIXzzzTcsXLgQt9uN3W5n+PDhXHrppWe0LxER8V+9i2LkyJF06tSJoKCgsxogNzeX4cOHk5iYyOrVq1m+fDlpaWm0bNmSCRMmEBkZyS+//MKTTz5Jbm7uWd23iIj8tnpPPXXr1o2amho+++wz3n//fQAqKiooLy8/owAlJSV07doVgO7du7Nx40YA2rdvT2RkJADx8fHU1tZS1wTP/y4icq6r94hiy5Yt5OTkkJCQwA8//MDQoUMpLS1l+fLlTJw48bQDxMfHk5+fT1JSEhs2bDhl8WzcuJGEhARatGhxyvvIy8sjLy8PgOzsbBwOx2nnsdvtZ7R9Q1Eu/yiXf5TLP80tV72L4rXXXuOhhx7isssuY+TIkQB07NiRH3/88Te3zcrK8p2i/FhpaWmMGTOG+fPn895779GrVy/s9uMjFRUVsXDhQiZPnvyr95+SkkJKSorv9pmcJ97RRM8zr1z+US7/KJd/zsVccXFxv7qu3kWxd+9eLrvssuM3ttvr9V2K3/r47JQpU4Aj01AFBQW+5eXl5Tz77LM88MADxMbG1jeqiIicRfU+RtGmTRu++uqr45Zt3ryZtm3bnlGA/fv3A0dOD7JkyRIGDx4MwOHDh8nOzuaOO+6gS5cuZ7QPERE5ffUeUQwfPpynnnqKnj17Ultby8svv8yXX37Jo48+ekYB1q1bx6pVqwBISkoiOTkZgJUrV1JaWsq7777Lu+++CxwZeYSHh5/R/kRExD82wzCM+v5wRUUFa9euZe/evTgcDi655BJWrlzJ+PHjGzKj30pKSk5723Nx7rEhKZd/lMs/yuUfy45R1NTUsHTpUnbu3MlFF13E7bffzoEDB3jjjTdYsmQJ/fr1O61QIiLyf8NvFsW8efPYsWMHPXr04KuvvuKXX36hpKSE/v37M3r0aF24SETkHPebRfH111/z9NNPEx4ezu9+9zvS09P5+9//TmJiYmPkExERi/3mp55cLpfvAHJUVBTBwcEqCRGRZuQ3RxQej4dvv/32uGUn3tbJ+kREzl2/WRTh4eHMnj3bdzs0NPS42zabjRkzZjRMOhERsdxvFsXMmTMbI4eIiDRRfl24SEREmh8VhYiImFJRiIiIKRWFiIiYUlGIiIgpFYWIiJhSUYiIiCkVhYiImFJRiIiIKRWFiIiYUlGIiIgpFYWIiJhSUYiIiCkVhYiImFJRiIiIKRWFiIiYUlGIiIgpFYWIiJhSUYiIiCkVhYiImFJRiIiIKRWFiIiYUlGIiIgpFYWIiJhSUYiIiCkVhYiImFJRiIiIKbvVAXbu3MncuXNxuVxER0eTkZFBSEiIb73T6eThhx/m9ttv55ZbbrEwqYhI82T5iCI3N5c777yTnJwckpKSWL58+XHrFyxYQM+ePS1KJyIilhdFSUkJXbt2BaB79+5s3LjRt+6LL74gJiaGNm3aWBVPRKTZs3zqKT4+nvz8fJKSktiwYQPl5eUAuFwu3n//fTIzM08aZZwoLy+PvLw8ALKzs3E4HKedx263n9H2DUW5/KNc/lEu/zS3XI1SFFlZWVRWVp60PC0tjTFjxjB//nzee+89evXqhd1+JNLixYu58cYbCQ4O/s37T0lJISUlxXfb6XSedlaHw3FG2zcU5fKPcvlHufxzLuaKi4v71XWNUhSZmZmm66dMmQIcmYYqKCgAYPv27WzcuJGFCxdy+PBhbDYbQUFB3HDDDQ2eV0RE/j/Lp572799PeHg4Xq+XJUuWMHjwYACeeOIJ388sXryY4OBglYSIiAUsL4p169axatUqAJKSkkhOTrY4kYiIHMvyohgyZAhDhgwx/ZnU1NRGSiMiIiey/OOxIiLStKkoRETElIpCRERMqShERMSUikJEREypKERExJSKQkRETKkoRETElIpCRERMqShERMSUikJEREypKERExJSKQkRETKkoRETElIpCRERMqShERMSUikJEREypKERExJSKQkRETKkoRETElIpCRERMqShERMSUikJEREypKERExJSKQkRETKkoRETElIpCRERMqShERMSUikJEREypKERExJSKQkRETKkoRETElIpCRERMqShERMSU3eoAO3fuZO7cubhcLqKjo8nIyCAkJASAn3/+mZdffpnq6mpsNhvTpk0jKCjI4sQiIs2L5UWRm5vL8OHDSUxMZPXq1Sxfvpy0tDQ8Hg8vvfQSY8eOpV27dhw8eBC73fK4IiLNjuVTTyUlJXTt2hWA7t27s3HjRgC+/vpr2rZtS7t27QBo2bIlAQGWxxURaXYs/xM9Pj6e/Px8kpKS2LBhA+Xl5QDs3r0bm83Gk08+yYEDB+jbty9Dhw495X3k5eWRl5cHQHZ2Ng6H47Tz2O32M9q+oSiXf5TLP8rln+aWq1GKIisri8rKypOWp6WlMWbMGObPn897771Hr169fNNLHo+HrVu3Mm3aNM477zyeeOIJEhISuOyyy066n5SUFFJSUny3nU7naWd1OBxntH1DUS7/KJd/lMs/52KuuLi4X13XKEWRmZlpun7KlCnAkWmogoICAKKioujatSthYWEA9OzZkx07dpyyKEREpOFYPum/f/9+ALxeL0uWLGHw4MEA9OjRg6KiImpqavB4PHz//fe0adPGyqgiIs2S5cco1q1bx6pVqwBISkoiOTkZgNDQUG688UYmTZqEzWajZ8+eXHHFFVZGFRFpliwviiFDhjBkyJBTruvXrx/9+vVr5EQiInIsy6eeRESkabMZhmFYHUJERJoujShOMHHiRKsjnJJy+Ue5/KNc/mluuVQUIiJiSkUhIiKmVBQnOPYb3k2JcvlHufyjXP5pbrl0MFtERExpRCEiIqZUFCIiYsryb2Y3FV999RXz58/H6/UyaNAgbr31VktyzJo1i4KCAsLDw8nJyQHg0KFDPP/88+zdu5fo6GgefvhhQkNDGzWX0+lk5syZVFZWYrPZSElJYciQIZZnq62t5e9//ztutxuPx0Pv3r1JTU2lrKyM6dOnc/DgQRISEhg3bpwlF77yer1MnDiRyMhIJk6c2CRyPfDAAwQHBxMQEEBgYCDZ2dmWv44Ahw8fZs6cORQVFWGz2RgzZgxxcXGW5iopKeH555/33S4rKyM1NZX+/ftb/nx98MEHrF69GpvNRnx8POnp6VRWVjbM+8sQw+PxGGPHjjVKS0uNuro645FHHjGKioosyfLdd98ZP/74ozF+/HjfsjfeeMNYunSpYRiGsXTpUuONN95o9FwVFRXGjz/+aBiGYVRVVRkZGRlGUVGR5dm8Xq9RXV1tGIZh1NXVGZMmTTJ++OEHIycnx/j8888NwzCM3NxcY9WqVY2a66gVK1YY06dPN6ZNm2YYhtEkcqWnpxv79+8/bpnVr6NhGMZLL71k5OXlGYZx5LU8dOhQk8h1lMfjMe69916jrKzM8lzl5eVGenq6UVNTYxjGkffVmjVrGuz9paknYPv27cTGxnLhhRdit9vp27cv+fn5lmRJTEw86S+T/Px8+vfvD0D//v0tyRYREUFCQgIA559/Pq1bt6aiosLybDabjeDgYODINUw8Hg82m43vvvuO3r17AzBgwABLnrPy8nIKCgoYNGgQAIZhNIlcp2L161hVVcX333/PwIEDgSMX4Lngggssz3WszZs3ExsbS3R0dJPI5fV6qa2txePxUFtbS6tWrRrs/aWpJ6CiooKoqCjf7aioKAoLCy1MdLz9+/cTEREBQKtWrXynZrdKWVkZO3bsoGPHjk0im9frZcKECZSWlnL99ddz4YUXEhISQmBgIACRkZFUVFQ0eq7XXnuNu+66i+rqagAOHjzYJHIBPPnkkwAMHjyYlJQUy1/HsrIywsLCmDVrFj///DMJCQmMGDHC8lzHWrduHddccw1g/f+TkZGR3HzzzYwZM4agoCB69OhBQkJCg72/VBT/x9hsNmw2m2X7d7lc5OTkMGLECEJCQo5bZ1W2gIAAnnnmGQ4fPsyzzz5LSUlJo2c40Zdffkl4eDgJCQl89913Vsc5TlZWFpGRkezfv5+pU6eedGUzK15Hj8fDjh07+POf/0ynTp2YP38+y5YtszzXUW63my+//JI77rjjpHVW5Dp06BD5+fnMnDmTkJAQnnvuOb766qsG25+KgiPNe/Ra3XBkyiAyMtLCRMcLDw9n3759REREsG/fPt9V/xqb2+0mJyeH6667jquvvrpJZQO44IIL6NatG9u2baOqqgqPx0NgYCAVFRWN/nr+8MMP/Pe//2XTpk3U1tZSXV3Na6+9ZnkuwLfP8PBwrrrqKrZv32756xgVFUVUVBSdOnUCoHfv3ixbtszyXEdt2rSJ9u3b06pVK8D69/3mzZuJiYnx7ffqq6/mhx9+aLD3l45RAB06dGD37t2UlZXhdrtZv349vXr1sjqWT69evfj0008B+PTTT7nqqqsaPYNhGMyZM4fWrVtz0003NZlsBw4c4PDhw8CRT0B98803tG7dmm7durFhwwYAPvnkk0Z/Pe+44w7mzJnDzJkzeeihh7j00kvJyMiwPJfL5fJNhblcLr755hvatm1r+evYqlUroqKifKPBzZs306ZNG8tzHXXstBNY/753OBwUFhZSU1ODYRi+56uh3l/6Zvb/KigoYMGCBXi9XpKTkxk2bJglOaZPn86WLVs4ePAg4eHhpKamctVVV/H888/jdDot+yje1q1b+dvf/kbbtm19w+w//elPdOrUydJsP//8MzNnzsTr9WIYBn369OG2225jz549TJ8+nUOHDtG+fXvGjRtHixYtGi3Xsb777jtWrFjBxIkTLc+1Z88enn32WeDIdM+1117LsGHDOHjwoOXvsZ07dzJnzhzcbjcxMTGkp6djGIbluVwuF+np6cyYMcM33doUnq/Fixezfv16AgMDadeuHX/5y1+oqKhokPeXikJERExp6klEREypKERExJSKQkRETKkoRETElIpCRERMqSjknDZz5kwWLVpkyb4Nw2DWrFmMHDmSSZMm+bXt2rVrmTp1agMlE/GPvpktjeqBBx6gpqaGGTNm+E7m9/HHH7N27Voee+wxa8OdZVu3buWbb75h9uzZvsd61JIlS1i6dClw5FxVbreboKAgAKKjo3nuuee47rrrGjXv4sWLKS0tJSMjo1H3K02fikIandfr5Z///KdlX2o8XV6vl4CA+g/Cj16r4MSSABg2bJjv8X/yySd8/PHHZGVlnbWsImeTikIa3S233ML777/P9ddfzwUXXHDcurKyMsaOHctbb73lOwvmY489xnXXXcegQYN8v1Q7dOjAJ598QmhoKOPGjWP37t28/fbb1NXVcddddzFgwADffR44cICsrCwKCwtp3749Y8eOJTo6GoBdu3bx6quv8tNPPxEWFsYf//hH+vbtCxyZtgoKCsLpdLJlyxYeffRRunfvflzeiooK5s6dy9atWwkNDWXo0KGkpKSwevVq5s2bh9vtZvjw4dx8882kpqbW+zk6sTxSU1MZNWoUH374IZWVlQwZMoQBAwYwY8YMioqK6NGjBxkZGb6L1Hz55ZcsWrSIvXv30qZNG+677z4uvvhiAJYtW8ZHH31EdXU1ERER3HvvvXg8Ht8IJz8/n9jYWJ555hmqqqpYsGABmzZtwmazkZycTGpqKgEBAb6M7dq147PPPiMiIoJRo0Zx2WWX+R7Du+++y4EDB2jZsiVpaWmNPkqSs0NFIY0uISGBbt26sWLFCtLS0vzevrCwkIEDB/Lqq6+yePFipk+fzpVXXsmLL77Ili1byMnJoXfv3r6/5D///HMmTpxIp06dePPNN3nxxRfJysrC5XIxdepUUlNT+Z//+R9++eUXpk6dStu2bWnTpo1v20mTJjFhwgTcbvdJWV544QXi4+PJzc2lpKSErKwsYmNjGThwIAEBAWd1pPD111+TnZ1NeXk5EyZMYNu2bYwbN46WLVsyefJkPv/8cwYMGMCOHTuYPXs2EyZMoEOHDnz22Wc8/fTTTJ8+nb1797Jq1SqmTZtGZGQkZWVleL1eYmNj+f3vf3/S1NPMmTMJDw/nxRdfpKamhuzsbKKiohg8eLDvtbj66quZN28eX3zxBc8++ywzZ87Ebrczf/58pk2bRlxcHPv27ePQoUNn5XmQxqeD2WKJ1NRUPvroIw4cOOD3tjExMSQnJxMQEEDfvn0pLy/ntttuo0WLFvTo0QO73U5paanv56+44goSExNp0aIFf/rTn9i2bRtOp5OCggKio6NJTk4mMDCQ9u3bc/XVV/Of//zHt+1VV11Fly5dCAgI8B1DOMrpdLJ161buvPNOgoKCaNeuHYMGDfKdLO5su+WWWwgJCSE+Pp74+Hi6d+/uu/ZGz5492blzJwB5eXmkpKTQqVMnAgICGDBgAHa7ncLCQgICAqirq6O4uNh3TqXY2NhT7q+yspJNmzYxYsQIgoODCQ8P58Ybb2T9+vW+nzm67OgFv+Li4igoKACOnH77l19+oba2loiICOLj4xvkeZGGpxGFWKJt27ZceeWVLFu2jNatW/u1bXh4uO/fR395Hz3989FlLpfLd/vYi1IFBwcTGhrKvn372Lt3L4WFhYwYMcK33uPx0K9fv1Nue6J9+/YRGhrK+eef71vmcDj48ccf/Xo89XXiYzzxdmVlJXCkwD799FNWrlzpW+92u6moqCAxMZERI0bwzjvvUFxcTI8ePbj77rtPeTpqp9OJx+Ph/vvv9y0zDOO45yQyMvK4azFER0dTUVFBcHAwDz30ECtWrGDOnDl07tyZu+++2+/XWpoGFYVYJjU1lQkTJhx32vKj00U1NTW+M3Ue/QV4uo691ojL5eLQoUNEREQQFRVFYmIimZmZv7qt2QVpIiIiOHToENXV1b6ycDqdll/LJCoq6riD5Se69tprufbaa6mqquLll19m4cKFjBs37qTHGhUVhd1uZ968eb7jRSeqqKjAMAzftk6n03dq68svv5zLL7+c2tpaFi1aRG5uLk888cRZfKTSWDT1JJaJjY2lT58+fPTRR75lYWFhREZGsnbtWrxeL6tXr2bPnj1ntJ9NmzaxdetW3G43ixYt4pJLLsHhcHDllVeye/duPvvsM9xuN263m+3bt1NcXFyv+3U4HHTu3Jl//OMf1NbW8vPPP7NmzRrLD9gOGjSIf//73xQWFmIYBi6Xi4KCAqqrqykpKeHbb7+lrq6OoKAggoKCfL/kw8PD2bt3L16vFzhShD169OD111+nqqoKr9dLaWkpW7Zs8e1r//79fPTRR7jdbv7zn/+wa9cuevbsSWVlJfn5+bhcLux2O8HBwZZemVHOjEYUYqnbbruNtWvXHrds9OjRvPLKK7z11lsMHDiQSy655Iz2cc011/DOO++wbds2EhISGDduHADnn38+U6ZMYcGCBSxYsADDMLj44ou555576n3fDz74IHPnzmX06NGEhoZy++23n/TJqMbWoUMHRo8ezauvvsru3bsJCgqiS5cudO3albq6OhYuXMiuXbsIDAykc+fOvqmlPn36sHbtWkaNGkVMTAxPPfUUY8eOZeHChYwfP57q6mouvPBChg4d6ttXp06d2L17N6NGjaJVq1aMHz+eli1bsm/fPj744ANmzJiBzWajXbt23HfffVY9JXKGdD0KETkt+v5H86GpJxERMaWiEBERU5p6EhERUxpRiIiIKRWFiIiYUlGIiIgpFYWIiJhSUYiIiKn/B3Qej8pGGlXpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timesteps = 5000\n",
    "controller = train(model, timesteps)\n",
    "plot_learning_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-georgia",
   "metadata": {},
   "source": [
    "### Run the model\n",
    "Run the trained model and save a trace of each episode to csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "after-evans",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output/behaviour_trace.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model( model, controller, 100, 'behaviour_trace.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-capture",
   "metadata": {},
   "source": [
    "### Next\n",
    "\n",
    "Got to notebook 'visualise'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-headquarters",
   "metadata": {},
   "source": [
    "### References\n",
    "Chen, X., Acharya, A., Oulasvirta, A., & Howes, A. (2021, May). An adaptive model of gaze-based selection. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (pp. 1-11)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
